[
  {
    "path": "posts/2024-12-21-introducing-pipeflow/",
    "title": "Introducing the {pipeflow} package",
    "description": "Efficiently managing complex data analysis workflows can be a challenge.\nIn standard R programming, chaining functions, tracking intermediate results,\nand maintaining dependencies between steps often lead to cluttered code that\nis difficult to scale or modify. Enter {pipeflow} — a beginner-friendly R\npackage designed to simplify and streamline data analysis pipelines by making\nthem modular, intuitive, and adaptable.",
    "author": [],
    "date": "2024-12-22",
    "categories": [
      "data analysis workflows",
      "pipeline tools",
      "reproducible research"
    ],
    "contents": "\r\n\r\nContents\r\nThe Problem: Standard R Workflow\r\nThe {pipeflow} Solution: Modular and Manageable\r\nWhy {pipeflow}?\r\nDynamic Updates\r\n{pipeflow} vs. targets\r\nConclusion\r\n\r\nIn this post, we’ll contrast the traditional approach with {pipeflow}, showcasing how it\r\nempowers users to build robust workflows while reducing complexity. Let’s dive in!\r\n\r\n\r\n\r\nThe Problem: Standard R Workflow\r\nConsider an analysis of R’s airquality data set,\r\n\r\n\r\nhead(airquality)\r\n\r\n  Ozone Solar.R Wind Temp Month Day\r\n1    41     190  7.4   67     5   1\r\n2    36     118  8.0   72     5   2\r\n3    12     149 12.6   74     5   3\r\n4    18     313 11.5   62     5   4\r\n5    NA      NA 14.3   56     5   5\r\n6    28      NA 14.9   66     5   6\r\n\r\nwhere we want to:\r\nAdd a new column, Temp.Celsius, converting temperatures from Fahrenheit to Celsius.\r\nFit a linear model predicting Ozone based on temperature.\r\nVisualize the data alongside the model fit.\r\nHere’s how the workflow might look using standard R:\r\n\r\n\r\nlibrary(ggplot2)\r\n\r\n# Step 1: Prepare the data\r\nairquality$Temp.Celsius <- (airquality$Temp - 32) * 5 / 9\r\n\r\n# Step 2: Fit a linear model\r\nmodel <- lm(Ozone ~ Temp.Celsius, data = airquality)\r\n\r\n# Step 3: Generate the plot\r\ncoeffs <- coefficients(model)\r\nggplot(airquality) +\r\n  geom_point(aes(Temp.Celsius, Ozone)) +\r\n  geom_abline(intercept = coeffs[1], slope = coeffs[2]) +\r\n  labs(title = \"Linear model fit\")\r\n\r\n\r\n\r\nWhile functional, this approach has clear drawbacks:\r\nManual Dependency Management: Each step depends on outputs from the previous\r\nsteps, which must be tracked manually.\r\nLimited Reusability: Modifying or extending the workflow requires substantial\r\neffort and risks introducing errors.\r\nCluttered Code: With all operations in a single script, the overall structure\r\nof the workflow becomes unclear.\r\nThe {pipeflow} Solution: Modular and Manageable\r\n{pipeflow} addresses these challenges by organizing workflows into modular,\r\ndependency-aware steps. Let’s rewrite the same workflow using {pipeflow}.\r\nStep 1: Initialize the Pipeline\r\nFirst, we create a pipeline named “my-pipeline” and load the airquality\r\ndataset as input:\r\n\r\n\r\nlibrary(pipeflow)\r\n\r\npip <- Pipeline$new(\"my-pipeline\", data = airquality)\r\n\r\n\r\nStep 2: Add a Data Preparation Step\r\nNext, we add a step to calculate Temp.Celsius:\r\n\r\n\r\npip$add(\r\n  \"data_prep\",\r\n  function(data = ~data) {\r\n    replace(data, \"Temp.Celsius\", (data[, \"Temp\"] - 32) * 5 / 9)\r\n  }\r\n)\r\n\r\n\r\nStep 3: Fit a Linear Model\r\nWe add another step to fit a linear model using the transformed data:\r\n\r\n\r\npip$add(\r\n  \"model_fit\",\r\n  function(data = ~data_prep, xVar = \"Temp.Celsius\") {\r\n    lm(paste(\"Ozone ~\", xVar), data = data)\r\n  }\r\n)\r\n\r\n\r\nStep 4: Visualize the Model Fit\r\nFinally, we create a visualization step that uses the outputs from model_fit\r\nand data_prep:\r\n\r\n\r\npip$add(\r\n  \"model_plot\",\r\n  function(\r\n    model = ~model_fit,\r\n    data = ~data_prep,\r\n    xVar = \"Temp.Celsius\",\r\n    title = \"Linear model fit\"\r\n  ) {\r\n    coeffs <- coefficients(model)\r\n    ggplot(data) +\r\n      geom_point(aes(.data[[xVar]], .data[[\"Ozone\"]])) +\r\n      geom_abline(intercept = coeffs[1], slope = coeffs[2]) +\r\n      labs(title = title)\r\n  }\r\n)\r\n\r\n\r\nNow, we can run the pipeline and inspect the model plot:\r\n\r\n\r\npip$run()\r\n\r\nINFO  [2024-12-23 10:32:08.415] Start run of 'my-pipeline' pipeline:\r\nINFO  [2024-12-23 10:32:08.447] Step 1/4 data\r\nINFO  [2024-12-23 10:32:08.458] Step 2/4 data_prep\r\nINFO  [2024-12-23 10:32:08.483] Step 3/4 model_fit\r\nINFO  [2024-12-23 10:32:08.487] Step 4/4 model_plot\r\nINFO  [2024-12-23 10:32:08.493] Finished execution of steps.\r\nINFO  [2024-12-23 10:32:08.494] Done.\r\n\r\npip$get_out(\"model_plot\")\r\n\r\n\r\n\r\nWhy {pipeflow}?\r\nHere are the key advantages of using {pipeflow} over the standard approach:\r\nAutomatic Dependency Management: Dependencies between steps are handled\r\nautomatically using the ~ operator to reference previous steps -> No\r\nmanual tracking of intermediate variables.\r\nModularity: Each step is independent, making pipelines easier to debug\r\nand modify.\r\nFlexibility: Parameters can be dynamically updated without affecting the\r\nrest of the pipeline and steps that depend on modified inputs are\r\nautomatically rerun.\r\nVisualization: {pipeflow} supports graphical representations of pipelines\r\nfor a clear view of the workflow structure.\r\nVisualizing the Pipeline\r\nWith {pipeflow}, you can easily visualize your pipeline using the visNetwork\r\npackage to produce a diagram showing the flow from data to data_prep,\r\nmodel_fit, and model_plot, making the workflow immediately understandable.\r\n\r\n\r\nlibrary(visNetwork)\r\ndo.call(visNetwork, args = pip$get_graph()) |>\r\n    visHierarchicalLayout(direction = \"LR\")\r\n\r\n\r\n\r\n\r\n\r\nEnsuring Integrity\r\n{pipeflow} also verifies pipeline integrity at definition time. For example,\r\ntrying to reference a non-existent step triggers an error:\r\n\r\n\r\npip$add(\r\n  \"invalid_step\",\r\n  function(data = ~non_existent) {\r\n    data\r\n  }\r\n)\r\n\r\nError: step 'invalid_step': dependency 'non_existent' not found\r\n\r\nThis proactive error-checking ensures that pipelines remain robust and free\r\nfrom misconfigurations.\r\nDynamic Updates\r\nOne of {pipeflow}’s standout features is its ability to dynamically update\r\nparameters and rerun only the affected steps. For example:\r\n\r\n\r\n# Change the predictor variable\r\npip$set_params(list(xVar = \"Solar.R\"))\r\npip$run()\r\n\r\nINFO  [2024-12-23 10:32:08.897] Start run of 'my-pipeline' pipeline:\r\nINFO  [2024-12-23 10:32:08.898] Step 1/4 data - skip 'done' step\r\nINFO  [2024-12-23 10:32:08.899] Step 2/4 data_prep - skip 'done' step\r\nINFO  [2024-12-23 10:32:08.900] Step 3/4 model_fit\r\nINFO  [2024-12-23 10:32:08.903] Step 4/4 model_plot\r\nINFO  [2024-12-23 10:32:08.912] Finished execution of steps.\r\nINFO  [2024-12-23 10:32:08.913] Done.\r\n\r\nOnly the steps depending on xVar (i.e., model_fit and model_plot)\r\nare rerun.\r\n\r\n\r\n# Update input data using only the first 10 rows\r\npip$set_data(airquality[1:10, ])\r\npip$run()\r\n\r\nINFO  [2024-12-23 10:32:08.941] Start run of 'my-pipeline' pipeline:\r\nINFO  [2024-12-23 10:32:08.942] Step 1/4 data\r\nINFO  [2024-12-23 10:32:08.945] Step 2/4 data_prep\r\nINFO  [2024-12-23 10:32:08.950] Step 3/4 model_fit\r\nINFO  [2024-12-23 10:32:08.952] Step 4/4 model_plot\r\nINFO  [2024-12-23 10:32:08.957] Finished execution of steps.\r\nINFO  [2024-12-23 10:32:08.958] Done.\r\n\r\nThe entire pipeline is rerun, as all steps depend on the input data.\r\n\r\n\r\n# Update the plot title\r\npip$set_params(list(title = \"Updated Plot Title\"))\r\npip$run()\r\n\r\nINFO  [2024-12-23 10:32:08.987] Start run of 'my-pipeline' pipeline:\r\nINFO  [2024-12-23 10:32:08.988] Step 1/4 data - skip 'done' step\r\nINFO  [2024-12-23 10:32:08.989] Step 2/4 data_prep - skip 'done' step\r\nINFO  [2024-12-23 10:32:08.990] Step 3/4 model_fit - skip 'done' step\r\nINFO  [2024-12-23 10:32:08.991] Step 4/4 model_plot\r\nINFO  [2024-12-23 10:32:08.996] Finished execution of steps.\r\nINFO  [2024-12-23 10:32:08.997] Done.\r\n\r\nOnly the model_plot step is rerun. Let’s inspect the final result with\r\nthe updated x-axis variable and plot title:\r\n\r\n\r\npip$get_out(\"model_plot\")\r\n\r\n\r\n\r\n{pipeflow} vs. targets\r\nThe R ecosystem includes powerful tools like\r\ntargets, designed for advanced,\r\nreproducible workflows. However,\r\ntargets\r\nmay involve additional setup and a steeper learning curve while\r\n{pipeflow} emphasizes simplicity:\r\nQuick Setup: Define and run a pipeline in just a few steps.\r\nIntuitive Design: Seamlessly manage dependencies without extra configuration.\r\nDynamic Updates: Modify parameters and inputs on the fly.\r\nWhile targets excels in highly complex workflows, {pipeflow} offers a\r\nversatile solution that’s both beginner-friendly but still capable of\r\nsupporting demanding tasks.\r\nConclusion\r\n{pipeflow} transforms the way you build and manage data analysis workflows in R.\r\nBy automating dependency tracking, ensuring pipeline integrity, and enabling\r\ndynamic updates, it reduces complexity and enhances productivity. Whether\r\nyou’re working on a simple analysis or a large-scale project, {pipeflow} helps\r\nyou focus on insights rather than infrastructure.\r\nReady to give {pipeflow} a try? Explore the\r\ndocumentation to learn more\r\nand start building smarter pipelines today!\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-12-21-introducing-pipeflow/logo.png",
    "last_modified": "2024-12-23T10:32:09+01:00",
    "input_file": "introducing-pipeflow.knit.md",
    "preview_width": 196,
    "preview_height": 145
  },
  {
    "path": "posts/2024-10-07-nested-unit-tests-with-testthat/",
    "title": "Nested unit tests with testthat",
    "description": "The testthat package is the most widely used tool for unit testing in R.\nHowever, many users may not be aware of the possibility to nest test blocks\nwithin each other. In this post, I demonstrate how this underused feature\nprovides a great way to structure and manage your unit tests.",
    "author": [],
    "date": "2024-10-07",
    "categories": [
      "testthat",
      "unit testing"
    ],
    "contents": "\r\n\r\n\r\n\r\nGrouping unit tests by function\r\nSay we have a function add that adds two numbers.\r\n\r\n\r\nadd <- function(a, b) {\r\n    stopifnot(\r\n        is.numeric(a),\r\n        is.numeric(b)\r\n    )\r\n    a + b\r\n}\r\n\r\n\r\nOur unit tests for this function could look like this:\r\n\r\n\r\nrequire(testthat)\r\n\r\ntest_that(\"add works as expected\", {\r\n  expect_equal(add(1, 2), 3)\r\n})\r\n\r\ntest_that(\"add signals bad input types\", {\r\n    expect_error(add(\"1\", 2), \"is.numeric(a) is not TRUE\", fixed = TRUE)\r\n    expect_error(add(1, \"2\"), \"is.numeric(b) is not TRUE\", fixed = TRUE)\r\n})\r\n\r\n\r\nNaturally, we want to group these tests by the function they test, which can\r\nbe done as follows:\r\n\r\n\r\ntest_that(\"add\", {\r\n\r\n    expect_true(exists(\"add\"))  # prevent \"empty test\" notification\r\n\r\n    test_that(\"works as expected\", {\r\n        expect_equal(add(1, 2), 3)\r\n    })\r\n\r\n    test_that(\"signals bad input types\", {\r\n        expect_error(add(\"1\", 2), \"is.numeric(a) is not TRUE\", fixed = TRUE)\r\n        expect_error(add(1, \"2\"), \"is.numeric(b) is not TRUE\", fixed = TRUE)\r\n    })\r\n})\r\n\r\n\r\nWe now have all tests related to the add function grouped nicely together.\r\nNote that we added one test at the top to prevent the “empty test” notification\r\nthat testthat would otherwise give us when running the entire block\r\nas part of a test suite.\r\nGrouping your tests this way becomes increasingly attractive when working on\r\nlarger projects. It will make it a lot easier to find the tests you are\r\nlooking for and to move around blocks of tests for specific functions.\r\nIt also allows to quickly skip all tests in case your code breaks and you\r\nneed to revise your function:\r\n\r\n\r\ntest_that(\"add\", {\r\n\r\n    expect_true(exists(\"add\"))  # prevent \"empty test\" notification\r\n\r\n    skip(\"for now - need to revise function\")   # <- skip all tests\r\n\r\n    test_that(\"works as expected\", {\r\n        expect_equal(add(1, 2), 3)\r\n    })\r\n\r\n    test_that(\"signals bad input types\", {\r\n        expect_error(add(\"1\", 2), \"is.numeric(a) is not TRUE\", fixed = TRUE)\r\n        expect_error(add(1, \"2\"), \"is.numeric(b) is not TRUE\", fixed = TRUE)\r\n    })\r\n})\r\n\r\n\r\nShortcut notation for tested functions\r\nThe block-like structure allows to take the testing one step further by\r\nalways using a shortcut notation for the function being tested:\r\n\r\n\r\ntest_that(\"add\", {\r\n\r\n    f <- add\r\n    expect_true(is.function(f))\r\n\r\n    test_that(\"works as expected\", {\r\n        expect_equal(f(1, 2), 3)\r\n    })\r\n\r\n    test_that(\"signals bad input types\", {\r\n        expect_error(f(\"1\", 2), \"is.numeric(a) is not TRUE\", fixed = TRUE)\r\n        expect_error(f(1, \"2\"), \"is.numeric(b) is not TRUE\", fixed = TRUE)\r\n    })\r\n})\r\n\r\n\r\nThis approach has several advantages as it makes it easier to\r\nspot your function when searching across files\r\nrename the function in the entire code base\r\nwrite compact test code when the function name is long\r\nre-use tests for similar functions\r\nLocal scoping\r\nAnother advantage of grouped unit tests is that it allows for re-using\r\nvariables or test data while keeping them local to the test block.\r\n\r\n\r\ntest_that(\"add\", {\r\n\r\n    f <- add\r\n    expect_true(is.function(f))\r\n\r\n    a <- 1\r\n    b <- 2\r\n    sum <- a + b\r\n\r\n    test_that(\"works as expected\", {\r\n        expect_equal(f(a, b), sum)\r\n    })\r\n\r\n    test_that(\"works with negative numbers\", {\r\n        expect_equal(f(-a, -b), -sum)\r\n    })\r\n\r\n    test_that(\"signals bad input types\", {\r\n        expect_error(f(\"a\", b), \"is.numeric(a) is not TRUE\", fixed = TRUE)\r\n        expect_error(f(a, \"b\"), \"is.numeric(b) is not TRUE\", fixed = TRUE)\r\n    })\r\n})\r\n\r\n\r\nGrouping unit tests by other criteria\r\nOf course, you can group your tests by any criteria you like. For example,\r\nyou could group them by the type of test, the expected outcome, or the\r\ninput data. As a default, I would recommend grouping by function, and then\r\nadd additional test blocks as needed. A common scenario occurs when you\r\nare performing regression tests, for example:\r\n\r\n\r\ntest_that(\"add function matches reference results\", {\r\n\r\n    f <- add\r\n    expect_true(is.function(f))\r\n\r\n    test_that(\"matches reference for positive numbers\", {\r\n        a <- 1\r\n        b <- 2\r\n        ref <- 3\r\n        expect_equal(f(a, b), ref)\r\n    })\r\n\r\n    test_that(\"matches reference for negative numbers\", {\r\n        a <- -1\r\n        b <- -2\r\n        ref <- -3\r\n        expect_equal(f(a, b), ref)\r\n    })\r\n})\r\n\r\n\r\nSummary\r\nIn Summary, I recommend to always group your unit tests at least by function\r\nas this will make your test suite much more readable and maintainable. Once\r\nyou get used to this structure, you will find even more ways to organize\r\nyour test suite.\r\nLet me know in the comments if you found this post helpful or if you\r\nhave any additional tips on this topic.\r\n\r\n\r\n\r\n",
    "preview": "posts/2024-10-07-nested-unit-tests-with-testthat/nested-tests.png",
    "last_modified": "2024-12-21T20:06:50+01:00",
    "input_file": {},
    "preview_width": 1024,
    "preview_height": 1024
  },
  {
    "path": "posts/readable-code-part2/",
    "title": "Readable code with base R (part 2)",
    "description": "It's been a while since my first on this topic.\nNevertheless, it remains an important issue as (in my humble view) there is\nstill too much code appearing in R packages that lacks good readability. \nSo I hope this post helps to promote the beauty of readable code.",
    "author": [],
    "date": "2022-06-26",
    "categories": [
      "base R",
      "coding style"
    ],
    "contents": "\r\n\r\n\r\n\r\nCombine startsWith and endsWith with R’s pipe\r\nIn my first post, startsWith and endsWith were presented. In combination with R’s pipe operator, we can improve the readability even further.\r\n\r\n\r\nw <- \"Hello World!\"\r\n\r\nw |> startsWith(\"Hell\")\r\n\r\n\r\n[1] TRUE\r\n\r\nw |> endsWith(\"!\")\r\n\r\n\r\n[1] TRUE\r\n\r\nProceeding with the example of the initial post, let’s see this in context of control flow.\r\n\r\n\r\ntell_file_type <- function(filename)\r\n{\r\n    if (filename |> endsWith(\"txt\"))\r\n        print(\"A text file.\")\r\n    \r\n    excel_endings <- c(\"xlsx\", \"xls\")\r\n    \r\n    if (any(filename |> endsWith(excel_endings)))\r\n        print(\"An Excel file.\")\r\n    \r\n}\r\ntell_file_type(\"A.xlsx\")\r\n\r\n\r\n[1] \"An Excel file.\"\r\n\r\n%in% and %not in%\r\nThe %in% operator is commonly used. To improve the readability of something like\r\n\r\n\r\nexisting_names <- c(\"Lisa\", \"Bob\")\r\n\r\nname <- \"Peter\"\r\n\r\nhasNewName = !(name %in% existing_names)\r\n\r\n\r\n\r\nyou can always define your own operators.\r\n\r\n\r\n`%not in%` <- Negate(`%in%`)\r\n\r\nhasNewName = name %not in% existing_names\r\n\r\n\r\n\r\nObviously, the readability also depends on the choice of proper variable names. My general take on this: don’t shy away from longish variable names, if it improves code readability.\r\nIn this context, it is often useful to assign boolean values to variables. For example, instead of\r\n\r\n\r\nif (abs_error < 1e-8) {\r\n    # ...\r\n}\r\n\r\n\r\n\r\nyou should do\r\n\r\n\r\nhasConverged <- abs_error < 1e-8\r\n\r\nif (hasConverged) {\r\n    # ...\r\n}\r\n\r\n\r\n\r\nThat is, it is ok to add redundancies in your code if it improves readability.\r\nSometimes, it is not immediately clear from the naming of base R functions, what they do. Of course, you are free to redefine them with a proper name.\r\n\r\n\r\nequals_pattern = function(x, pattern, ...) grepl(pattern, x, ...)\r\n\r\nx <- \"Peter\"\r\n\r\nx |> equals_pattern(\"^P\")\r\n\r\n\r\n[1] TRUE\r\n\r\nLastly, let’s combine all of the above.\r\n\r\n\r\nx |> equals_pattern(\"^P\") && \r\nx |> startsWith(\"z\")      &&\r\nx %not in% existing_names \r\n\r\n\r\n[1] FALSE\r\n\r\nThat’s it for now. Some of the above examples may or may not appear a bit artificial, which can happen with examples, I guess. Let me know in the comments how you think about this topic or maybe you have another good example of readable code.\r\n\r\n\r\n\r\n",
    "preview": "posts/readable-code-part2/distill-preview.png",
    "last_modified": "2024-12-21T20:06:50+01:00",
    "input_file": {},
    "preview_width": 569,
    "preview_height": 322
  },
  {
    "path": "posts/container1.0/",
    "title": "container: v1.0.0 on CRAN",
    "description": "The update contains some breaking changes and brings a lot of new features\nand operators, which markedly improves using containers in both\ninteractive R sessions and code development.\nAlso there is a new class dict.table to support the data.table package.",
    "author": [],
    "date": "2021-12-18",
    "categories": [
      "container",
      "list",
      "R package"
    ],
    "contents": "\r\n\r\nContents\r\nWhat is container?\r\nPrint\r\nExtract\r\nReplace\r\n\r\nWhat is container?\r\n\r\nA container can be considered as an enhanced version of base R’s list with a carefully designed set of extract, replace, and remove operations. They enable an easier and safer way to work with list-like data structures in interactive R sessions and in particular when developing critical code.\r\n\r\n\r\n\r\n\r\nThis blog post can only provide a glimpse of all the things you can do with containers so here the focus is on some features suitable for interactive R sessions.\r\nPrint\r\n\r\n\r\nlibrary(container)\r\n\r\nco <- container(colors = c(\"Red\", \"Green\", \"Blue\"),\r\n                numbers = 1:10,\r\n                data = cars[1:10, ])\r\n\r\n\r\n\r\nLet’s start by comparing the print output of container and list (click on the panels below).\r\n\r\ncontainer\r\n\r\n\r\nprint(co)\r\n\r\n\r\n[colors = (\"Red\" \"Green\" \"Blue\"), numbers = (1L 2L 3L 4L ...),\r\n data = <<data.frame(10x2)>>]\r\n\r\nlist\r\n\r\n\r\nli = as.list(co)\r\nprint(li)\r\n\r\n\r\n$colors\r\n[1] \"Red\"   \"Green\" \"Blue\" \r\n\r\n$numbers\r\n [1]  1  2  3  4  5  6  7  8  9 10\r\n\r\n$data\r\n   speed dist\r\n1      4    2\r\n2      4   10\r\n3      7    4\r\n4      7   22\r\n5      8   16\r\n6      9   10\r\n7     10   18\r\n8     10   26\r\n9     10   34\r\n10    11   17\r\n\r\n\r\nAs you can see the container by default prints very compact. Next, take a look at data extraction.\r\nExtract\r\nA container provides all familiar extract operations known from base R lists but also some new operations to make life (hopefully) easier. In contrast to base R lists, undefined indices are ignored and thus do not produce NULL values.\r\n\r\ncontainer\r\n\r\n\r\nco[[1]]                  # standard\r\n\r\n\r\n[1] \"Red\"   \"Green\" \"Blue\" \r\n\r\nco[2:3]                  # standard\r\n\r\n\r\n[numbers = (1L 2L 3L 4L ...), data = <<data.frame(10x2)>>]\r\n\r\nco[c(1:2, 5:8)]          # ignore undefined indices\r\n\r\n\r\n[colors = (\"Red\" \"Green\" \"Blue\"), numbers = (1L 2L 3L 4L ...)]\r\n\r\nco[1, 2, 5:8]            # pass any number of indices - same as before\r\n\r\n\r\n[colors = (\"Red\" \"Green\" \"Blue\"), numbers = (1L 2L 3L 4L ...)]\r\n\r\nco[1, \"data\", 2, \"foo\"]  # index types can be mixed\r\n\r\n\r\n[colors = (\"Red\" \"Green\" \"Blue\"), data = <<data.frame(10x2)>>,\r\n numbers = (1L 2L 3L 4L ...)]\r\n\r\nlist\r\n\r\n\r\nli[[1]]                  # standard\r\n\r\n\r\n[1] \"Red\"   \"Green\" \"Blue\" \r\n\r\nli[2:3]                  # standard\r\n\r\n\r\n$numbers\r\n [1]  1  2  3  4  5  6  7  8  9 10\r\n\r\n$data\r\n   speed dist\r\n1      4    2\r\n2      4   10\r\n3      7    4\r\n4      7   22\r\n5      8   16\r\n6      9   10\r\n7     10   18\r\n8     10   26\r\n9     10   34\r\n10    11   17\r\n\r\nli[c(1:2, 5:8)]          # pad NULLs for undefined indices\r\n\r\n\r\n$colors\r\n[1] \"Red\"   \"Green\" \"Blue\" \r\n\r\n$numbers\r\n [1]  1  2  3  4  5  6  7  8  9 10\r\n\r\n$<NA>\r\nNULL\r\n\r\n$<NA>\r\nNULL\r\n\r\n$<NA>\r\nNULL\r\n\r\n$<NA>\r\nNULL\r\n\r\nli[1, 2, 5:8]            # not supported\r\n\r\n\r\nError in li[1, 2, 5:8]: incorrect number of dimensions\r\n\r\nli[1, \"data\", 2, \"foo\"]  # not supported\r\n\r\n\r\nError in li[1, \"data\", 2, \"foo\"]: incorrect number of dimensions\r\n\r\n\r\nReplace\r\nIn the same way, a container provides both familiar and new operations for interactive element replacement.\r\n\r\ncontainer\r\n\r\n\r\nco[2:3] <- NA                                # standard\r\nco\r\n\r\n\r\n[colors = (\"Red\" \"Green\" \"Blue\"), numbers = NA, data = NA]\r\n\r\nco[[1]] <- \"\"                                # standard\r\nco\r\n\r\n\r\n[colors = \"\", numbers = NA, data = NA]\r\n\r\nco[[\"colors\"]] <- \"red\"                      # standard\r\nco\r\n\r\n\r\n[colors = \"red\", numbers = NA, data = NA]\r\n\r\nco[list(\"colors\", 2)] <- list(\"blue\", 1:4)   # mixed indices\r\nco\r\n\r\n\r\n[colors = \"blue\", numbers = (1L 2L 3L 4L), data = NA]\r\n\r\nco[[{\"blue\"}]] <- \"green\"                    # replace by value\r\nco\r\n\r\n\r\n[colors = \"green\", numbers = (1L 2L 3L 4L), data = NA]\r\n\r\nco[[{NA}]] <- 0                              # replace by value\r\nco\r\n\r\n\r\n[colors = \"green\", numbers = (1L 2L 3L 4L), data = 0]\r\n\r\nlist\r\n\r\n\r\nli[2:3] <- NA                                # standard\r\nli\r\n\r\n\r\n$colors\r\n[1] \"Red\"   \"Green\" \"Blue\" \r\n\r\n$numbers\r\n[1] NA\r\n\r\n$data\r\n[1] NA\r\n\r\nli[[1]] <- \"\"                                # standard\r\nli\r\n\r\n\r\n$colors\r\n[1] \"\"\r\n\r\n$numbers\r\n[1] NA\r\n\r\n$data\r\n[1] NA\r\n\r\nli[[\"colors\"]] <- \"red\"                      # standard\r\nco\r\n\r\n\r\n[colors = \"green\", numbers = (1L 2L 3L 4L), data = 0]\r\n\r\nli[list(\"colors\", 2)] <- list(\"blue\", 1:4)   # not supported\r\n\r\n\r\nError in li[list(\"colors\", 2)] <- list(\"blue\", 1:4): invalid subscript type 'list'\r\n\r\n#li[[{\"blue\"}]] <- \"green\"                   # not supported\r\n\r\n#li[[{NA}]] <- 0                             # not supported\r\n\r\n\r\n\r\n\r\nTo see the full official documentation, visit https://rpahl.github.io/container/.\r\n\r\n\r\n\r\n",
    "preview": "posts/container1.0/distill-preview.png",
    "last_modified": "2022-06-17T15:30:57+02:00",
    "input_file": {},
    "preview_width": 602,
    "preview_height": 415
  },
  {
    "path": "posts/ref-label/",
    "title": "Why I don't use R Markdown's ref.label",
    "description": "R Markdown provides the chunk option `ref.label` to \nreuse chunks. In this post, I'll show potential problems with this approach\nand present an easy and safe alternative.",
    "author": [],
    "date": "2020-08-08",
    "categories": [
      "R-Markdown",
      "knitr"
    ],
    "contents": "\r\n\r\n\r\n\r\nMotivation\r\nConsider you have defined variable x,\r\n\r\n\r\nx = 1\r\n\r\n\r\n\r\nand define another chunk, where you simply add one up\r\n\r\n```{r addOne}\r\nsum = x + 1\r\nsum\r\n```\r\n\r\nresulting in\r\n\r\n[1] 2\r\n\r\nTo reuse this chunk, an empty code block is created referencing the above chunk\r\n\r\n```{r, ref.label = 'addOne'}\r\n```\r\n\r\nagain resulting in\r\n\r\n\r\nsum = x + 1\r\nsum\r\n\r\n\r\n[1] 2\r\n\r\nBehind the scenes, the chunk basically was copy-pasted and then executed again. One problem is that one can easily lose track of the scope of the variables used in that chunk. For example, let’s assume you use the sum variable further below in your document to store some other result:\r\n\r\n\r\nsum = 10\r\n\r\n\r\n\r\nIf you now again reuse the above chunk\r\n\r\n```{r, ref.label = 'addOne'}\r\n```\r\n\r\n\r\n\r\nsum = x + 1\r\nsum\r\n\r\n\r\n[1] 2\r\n\r\nsum has been overwritten by the chunk:\r\n\r\n\r\nprint(sum)  # expect sum == 10\r\n\r\n\r\n[1] 2\r\n\r\nSince the ref.label chunk is empty, this issue might not be easily spotted.\r\nAnother inconvenience arrises with RStudio’s notebook functionality to execute individual code chunks. While the original chunk can be executed, none of the empty ref.label chunks can. Funnily enough, this inconvenience was what made me think about an alternative solution.\r\nAlternative solution\r\nLuckily, the solution is quite simple - put your entire chunk inside a function and then “reference” the function:\r\n\r\n\r\nadd1 <- function(x) {\r\n    sum = x + 1\r\n    sum\r\n}\r\n\r\n\r\n\r\n\r\n\r\nadd1(x)\r\n\r\n\r\n[1] 2\r\n\r\nNow both the sum variable is perfectly scoped and the “referenced” call can be executed in the RStudio notebook as usual. Plus, of course, this “chunk” could be easily parametrized:\r\n\r\n\r\naddY <- function(x, y) {\r\n    sum = x + y\r\n    sum\r\n}\r\naddY(x, y = 1)\r\n\r\n\r\n[1] 2\r\n\r\nSummary\r\nDownsides of using ref.label:\r\npotential issues with (global) variables as chunk does not provide local scoping\r\nref.label chunks are empty and therefore cannot be executed in RStudio notebooks\r\nProposed solution: encapsulate entire chunk inside a function and then execute the function wherever you would reference the chunk.\r\n\r\n\r\n\r\n",
    "preview": "posts/ref-label/distill-preview.png",
    "last_modified": "2022-06-17T15:30:58+02:00",
    "input_file": {},
    "preview_width": 279,
    "preview_height": 117
  },
  {
    "path": "posts/kable_if/",
    "title": "Quicker knitr kables in RStudio notebook",
    "description": "In this post a simple RStudio hack is presented on how to display tables\nproduced via knitr kable efficiently in the RStudio session.",
    "author": [],
    "date": "2019-11-17",
    "categories": [
      "R-Markdown",
      "knitr",
      "R-Studio"
    ],
    "contents": "\r\nThe setup\r\nThe RStudio notebook is a great interactive tool to build a statistical report. Being able to see statistics and graphs right on the fly probably has saved me countless hours, especially when building complex reports.\r\nHowever, one thing that has always bothered me was the way tables are displayed in the notebook with knitr’s kable function. For example, consider the airquality data set:\r\n\r\n\r\nhead(airquality)\r\n\r\n\r\n  Ozone Solar.R Wind Temp Month Day\r\n1    41     190  7.4   67     5   1\r\n2    36     118  8.0   72     5   2\r\n3    12     149 12.6   74     5   3\r\n4    18     313 11.5   62     5   4\r\n5    NA      NA 14.3   56     5   5\r\n6    28      NA 14.9   66     5   6\r\n\r\nTo get a nice table in your report you type\r\n\r\n\r\nknitr::kable(head(airquality), caption = \"New York Air Quality Measurements.\")\r\n\r\n\r\n\r\nwhich shows up nicely formatted in the final output\r\n\r\nTable 1: New York Air Quality Measurements.\r\nOzone\r\nSolar.R\r\nWind\r\nTemp\r\nMonth\r\nDay\r\n41\r\n190\r\n7.4\r\n67\r\n5\r\n1\r\n36\r\n118\r\n8.0\r\n72\r\n5\r\n2\r\n12\r\n149\r\n12.6\r\n74\r\n5\r\n3\r\n18\r\n313\r\n11.5\r\n62\r\n5\r\n4\r\nNA\r\nNA\r\n14.3\r\n56\r\n5\r\n5\r\n28\r\nNA\r\n14.9\r\n66\r\n5\r\n6\r\n\r\n\r\nThe problem\r\nBut in the interactive RStudio notebook session the table looks something like the following:\r\n\r\n\r\n\r\nSo first of all, the formatting is not that great. Secondly, the table chunk consumes way too much space of the notebook and, at times, can be very cumbersome to scroll. Also for bigger tables (and depending on your hardware) it can take up to a few seconds for the table to be built.\r\nSo often when I was using kable, I felt my workflow being disrupted. In the interactive session I want a table being built quickly and in a clean format. Now, using the simple print function you’ll get exactly this\r\n\r\n\r\n\r\nSo my initial quick-and-dirty workaround during the interactive session was to comment out the knitr statement and use the print function.\r\n\r\n\r\n#knitr::kable(head(airquality), caption = \"New York Air Quality Measurements.\")\r\nprint(head(airquality))\r\n\r\n\r\n\r\nThen, only when creating the final report, I would comment out the print function and use kable again. Of course, there is a much more elegant and easier solution to get this without having to switch between functions.\r\nThe solution\r\nWe define a simple wrapper, which chooses the corresponding function depending on the context:\r\n\r\n\r\nkable_if <- function(x, ...) if (interactive()) print(x, ...) else knitr::kable(x, ...)\r\n\r\n\r\n\r\nThen you simply call it as you would invoke kable and now you get both, the quick table in the interactive session …\r\n\r\n\r\n\r\n… and a formatted table in the report.\r\n\r\n\r\nkable_if(head(airquality), caption = \"New York Air Quality Measurements.\")\r\n\r\n\r\nTable 2: New York Air Quality Measurements.\r\nOzone\r\nSolar.R\r\nWind\r\nTemp\r\nMonth\r\nDay\r\n41\r\n190\r\n7.4\r\n67\r\n5\r\n1\r\n36\r\n118\r\n8.0\r\n72\r\n5\r\n2\r\n12\r\n149\r\n12.6\r\n74\r\n5\r\n3\r\n18\r\n313\r\n11.5\r\n62\r\n5\r\n4\r\nNA\r\nNA\r\n14.3\r\n56\r\n5\r\n5\r\n28\r\nNA\r\n14.9\r\n66\r\n5\r\n6\r\n\r\nThat’s it. Simply put this function definition somewhere in the top of your document and enjoy a quick workflow.\r\n\r\n\r\n\r\n",
    "preview": "posts/kable_if/distill-preview.png",
    "last_modified": "2022-06-17T15:30:58+02:00",
    "input_file": {},
    "preview_width": 670,
    "preview_height": 468
  },
  {
    "path": "posts/r-style-guide/",
    "title": "My R Style Guide",
    "description": "This is my take on an R style guide. As such, this is going to be a longer \npost in the hope that is useful to some people out there.",
    "author": [],
    "date": "2019-11-16",
    "categories": [
      "coding style"
    ],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nCoding style\r\nNotation and naming\r\nFile names\r\nFunction names\r\nVariable names\r\nFunction definitions\r\nFunction calls\r\nNumber of function arguments\r\n\r\nSyntax\r\nAssignment\r\nSpacing around …\r\nIndentation\r\n\r\nCode organization\r\nLine length\r\nBlock length\r\nPackages and namespaces\r\n\r\n\r\nCode documentation\r\nFunction headers\r\nInline code comments\r\n\r\n\r\n\r\n\r\n\r\nIntroduction\r\nThere are universal views about readability due to the way how humans process information or text. For example, consider the following number written in two ways:\r\n\r\n823969346\r\n823 969 346\r\n\r\nCertainly the second version, which splits the sequence into groups of numbers, is easier to process by humans, implying that spacing is important especially if abstract information is presented.\r\nThe style guide at hand provides a set of rules designed to achieve readable and maintainable R code. Still, of course, it represents a subjective view (of the author) on how to achieve these goals and does not raise any claims of being complete. Thus, if there are viable alternatives to the presented rules or if they are against the intuition of the user, possibly even resulting in hard-to-read code, it is better to deviate from the rules rather than blindly following them.\r\nCoding style\r\nNotation and naming\r\nFile names\r\nFile names end in .R and are meaningful about their content:\r\nGood:\r\nstring-algorithms.R\r\nutility-functions.R\r\nBad:\r\nfoo.R\r\nfoo.Rcode\r\nstuff.R\r\nFunction names\r\nPreferrably function names consist of lowercase words separated by an underscore. Using dot (.) separator is avoided as this confuses with the use of generic (S3) functions. It also prevents name clashes with existing functions from the standard R packages. Camel-case style is also suitable especially for predicate functions returning a boolean value. Function names ideally start with verbs and describe what the function does.\r\n\r\n\r\n# GOOD\r\ncreate_summary()\r\ncalculate_avg_clicks()\r\nfind_string()\r\nisOdd()\r\n\r\n# BAD\r\ncrt_smmry()\r\nfind.string()\r\nfoo()\r\n\r\n\r\n\r\nVariable names\r\nVariable names consist of lowercase words separated by an underscore or dot. Camel-case style is also suitable especially for variables representing boolean values. Variable names ideally are attributed nouns and describe what (state) they store.\r\nGood:\r\n\r\n\r\nsummary_tab\r\nselected_timeframe\r\nout.table\r\nhasConverged\r\n\r\n\r\n\r\nBad:\r\n\r\n\r\nsmrytab\r\nselTF\r\nouttab\r\nhascnvrgd\r\n\r\n\r\n\r\nName clashes with existing R base functions must be avoided:\r\n\r\n\r\n# Very bad:\r\nT <- FALSE\r\nc <- 10\r\nmean <- function(a, b) (a + b) / 2\r\nfile.path <- \"~/Downloads\" # clashes with base::file.path\r\n\r\n\r\n\r\nLoop variables or function arguments can be just single letters if\r\nthe naming follows standard conventions\r\ntheir meaning is clear\r\nunderstanding is preserved\r\notherwise use longer variable names.\r\n\r\n# GOOD\r\nfor (i in 1:10) print(i)\r\nadd <- function(a, b) a + b\r\nrnorm <- function(n, mean = 0, sd = 1)\r\n\r\n# BAD\r\nfor (unnecessary_long_variable_name in 1:10) {\r\n  print(unnecessary_long_variable_name)\r\n}\r\nadd <- function(a1, x7) a1 + x7\r\nrnorm <- function(m, n = 0, o = 1)\r\n\r\nFunction definitions\r\nFunction definitions first list arguments without default values, followed by those with default values. In both function definitions and function calls, multiple arguments per line are allowed; line breaks are only allowed between assignments.\r\n\r\n# GOOD\r\nrnorm <- function(n, mean=0, sd=1)\r\npnorm <- function(q, mean=0, sd=1, \r\n                  lower.tail=TRUE,\r\n                  log.p=FALSE)\r\n\r\n# BAD\r\nmean <- function(mean=0, sd=1, n)                   # n should be listed first\r\npnorm <- function(q, mean=0, sd=1, lower.tail=      # don't break line here\r\n                  TRUE, log.p=FALSE)\r\n\r\nFunction calls\r\nWhen calling a function, the meaning of the function call and arguments should be clear from the call, that is, usually function arguments beyond the first are explicitly named or at least invoked with a meaningful variable name, for example, identical to the name of the function argument:\r\n\r\n\r\n# GOOD\r\nrnorm(1, mean=1, sd=2)\r\nidentical(1, 1.0)   # no need for explicit naming as meaning of call is clear\r\n\r\nmean <- 1\r\nsd <- 2\r\nstd.dev <- sd\r\nrnorm(1, mean, sd)\r\nrnorm(1, mean, std.dev)\r\n\r\n# BAD\r\nrnorm(1, 1, 1)\r\n\r\n\r\n\r\nNumber of function arguments\r\nTry to limit the number of function arguments to around five. No human being is able to remember much more than this. If needed, group semantically similar arguments in lists or (even better) in functions returning a list. If you have a hard time doing this, you should think about how to split up your function as it probably does too many things.\r\n\r\n# GOOD\r\nmy_plot <- function(x, y, \r\n                   colors = list(bg = \"red\", fg = \"green\", line = \"black\"),\r\n                   linetype = list(main = 1, limit = 2, circle = 3))\r\n{ ... }\r\n\r\n# Even better\r\nplot_colors <- function(bg = \"red\", fg = \"green\", line = \"black\") {\r\n    list(bg = bg, fg = fg, line = line)\r\n}\r\n\r\nplot_linetypes <- function(main = 1, limit = 2, circle = 3) {\r\n    list(main = main, limit = limit, circle = circle)\r\n}\r\n\r\nmy_plot <- function(x, y, colors = plot_colors(), \r\n                    linetype = plot_linetypes()) { ... }\r\n\r\n\r\n# BAD\r\nmy_plot <- function(x, y, \r\n                   color.bg = \"red\", color.fg = \"green\", color.line = \"black\",\r\n                   linetype.main = 1, linetype.limit = 2, linetype.circle = 3)\r\n{ ... }\r\n\r\n# Even worse\r\nmy_plot <- function(x, y, cl.bg = \"red\", cl.fg = \"green\", cl.l = \"black\",\r\n                    lt.m = 1, lt.l = 2, lt.c = 3)\r\n{ ... }\r\n\r\n# Very bad\r\nmy_fit_and_plot <- function(formula, data, sub, w, na.action, method,\r\n                            x, y, cl.bg = \"red\", cl.fg = \"green\",\r\n                            cl.l = \"black\", lt.m = 1, lt.l = 2, lt.c = 3)\r\n\r\nSyntax\r\nAssignment\r\nFor assignment, both the arrow <- and equal sign = can be used. However, try to be consistent throughout.\r\nSemicolons should never used.\r\n\r\n\r\n# BAD\r\nx <- 5; y <- 10; z <- 3  # break into three lines instead\r\n\r\n\r\n\r\nSpacing around …\r\n… commas\r\nPlace a space after a comma but never before (as in regular English)\r\n\r\n\r\n# GOOD\r\nv <- c(1, 2, 3)\r\nm[1, 2]\r\n\r\n# BAD\r\nv <- c(1,2,3)\r\nm[1 ,2]\r\n\r\n\r\n\r\n… operators\r\nSpaces around infix operators (=, +, -, <-, etc.) should be done in a way that supports readability, for example, by placing spaces between semantically connected groups. If in doubt, rather use more spaces, except with colons :, which usually should not be surrounded by spaces.\r\n\r\n\r\n# GOOD\r\n# Spacing according to semantically connected groups\r\nx <- 1:10\r\nbase::get\r\naverage <- mean(feet/12 + inches, na.rm=TRUE)\r\n\r\n# Using more spaces - also ok\r\naverage <- mean(feet / 12 + inches, na.rm = TRUE)\r\n\r\n\r\n\r\n\r\n\r\n# BAD\r\nx <- 1 : 10\r\nbase :: get\r\naverage<-mean(feet/12+inches,na.rm=TRUE)\r\n\r\n\r\n\r\n… parentheses\r\nA space is placed before left parentheses, except in a function call, and after right parentheses. Arithmetic expressions form a special case, in which spaces can be omitted.\r\n\r\n\r\n# GOOD\r\nif (debug) print(x)\r\nplot(x, y)\r\n\r\n# Special case arithmetic expression:\r\n2 + (a+b)/(c+d) + z/(1+a)\r\n\r\n# BAD\r\nif(debug)print (x)\r\nplot (x, y)\r\n\r\n\r\n\r\nNo spaces are placed around code in parentheses or square brackets, unless there is a comma:\r\n\r\n\r\n# GOOD\r\nif (debug) print(x)\r\ndiamonds[3, ]\r\ndiamonds[, 4]\r\n\r\n# BAD\r\nif ( debug ) print( x )\r\ndiamonds[ ,4]\r\n\r\n\r\n\r\n… curly braces\r\nAn opening curly brace is followed by a new line. A closing curly brace goes on its own line.\r\n\r\n\r\n# GOOD\r\nfor (x in letters[1:10]) {\r\n    print(x)\r\n}\r\n\r\nadd <- function(x, y) {\r\n    x + y\r\n}\r\n\r\nadd <- function(x, y)\r\n{\r\n    x + y\r\n}\r\n\r\n# BAD\r\nadd <- function(x, y) {x + y}\r\n\r\n\r\n\r\nIndentation\r\nCode is indented with ideally four, but at least two spaces. Usually using four spaces provides better readability than two spaces especially the longer the indented code-block gets.\r\n\r\n\r\n# Four-space indent:\r\nfor (i in seq_len(10)) {\r\n    if (i %% 2 == 0) {\r\n        print(\"even\")\r\n    } else {\r\n        print(\"odd\")\r\n    }\r\n}\r\n\r\n# The same code-block using two-space indent:\r\nfor (i in seq_len(10)) {\r\n  if (i %% 2 == 0) {\r\n    print(\"even\")\r\n  } else {\r\n    print(\"odd\")\r\n  }\r\n}\r\n\r\n\r\n\r\nExtended indentation: when a line break occurs inside parentheses, align the wrapped line with the first character inside the parenthesis:\r\n\r\n\r\nfibonacci <- c(1, 1, 2, 3, 5,\r\n               8, 13, 21, 34)\r\n\r\n\r\n\r\nCode organization\r\nAs with a good syntax style, the main goal of good code organization is to provide good readability and understanding of the code, especially for external readers/reviewers. While the following guidelines generally have proven to be effective for this purpose, they harm things if applied the wrong way or in isolation. For example, if the user wants to restricts himself to 50 lines of code for each block (see below), but, instead of proper code-reorganization, achieves this by just deleting all comments in the code block, things probably have gotten worse. Thus, any (re-)organization of code first and foremost must serve the improvement of the readability and understanding of the code, ideally implemented by the guidelines given in this section.\r\nLine length\r\nIdeally, the code does not exceed 80 characters per line. This fits comfortably on a printed page with a reasonably sized font and therefore can be easily processed by a human, which tend to read line by line. Longer comments are simply broken into several lines:\r\n\r\n\r\n# Here is an example of a longer comment, which is just broken into two lines\r\n# in order to serve the 80 character rule.\r\n\r\n\r\n\r\nLong variable names can cause problems regarding the 80 characters limit. In such cases, one simple yet effective solution is to use interim results, which are saved in a new meaningful variable name. This at the same time often improves the readability of the code. For example:\r\n\r\n\r\n# Longer statement\r\ntotal_cost <- cost.hotel + cost.taxi + cost.lunch + cost.airplane +\r\n    cost.breakfast + cost.dinner + cost.car_rental\r\n\r\n# Solution with interim result\r\ncost.travel <- cost.taxi + cost.airplane + cost.car_rental\r\ncost.food <- cost.breakfast + cost.lunch + cost.dinner\r\ntotal_cost <- cost.travel + cost.food + cost.hotel\r\n\r\n\r\n\r\nSimilarly, four-space indenting in combination with multiple nested code-blocks can cause problems to maintain the 80 character limit and may require to relax this rule in such cases. At the same time, however, multiple nested code-blocks should be avoided in the first place, because with more nesting the code usually gets harder to understand.\r\nBlock length\r\nEach functionally connected block of code (usually a function block) should not exceed a single screen (about 50 lines of code). This allows the code to be read and understood without having to line-scroll. Exceeding this limit usually is a good indication that some of the code should be encapsulated (refactorized) into a separate unit or function. Doing so, not only improves the readability of the code but also flexibilizes (and thereby simplifies) further code development. In particular, single blocks that are separated by comments, often can be refactorized into functions, named similar to the comment, for example:\r\nLong single-block version:\r\n\r\n# Sub-block 1: simulate data for some model\r\nx <- 1:100\r\ny <- rnorm(length(x))\r\n.\r\n.\r\n.\r\nlonger code block generating some data\r\n.\r\n.\r\n.\r\ndata <- ...\r\n\r\n# Sub-block 2: plot the resulting data points\r\nylims <- c(0, 30)\r\np <- ggplot(data) +\r\n.\r\n.\r\n.\r\nlonger code block defining plot object\r\n.\r\n.\r\n.\r\n\r\n# Sub-block 3: format results and export to Excel file\r\noutFile <- \"output.xlsx\"\r\n.\r\n.\r\nexport to Excel file\r\n.\r\n.\r\n\r\nThe singe-block version may exceed a single page and requires a lot of comments just to separate each step visually, but even with this visual separation, it will be unnecessary difficult for a second person to understand the code, because allthough the code might be entirely sequential, he possibly will end up jumping back and forth within the block to get an understanding of it. In addition, if parts of the block are changed at a later time point, the code can easily get out of sync with the comments.\r\nRefactorized version:\r\n\r\n\r\n# Simulate data, plot it and export it to Excel file\r\ndata.sim <- simulate_data(x = 1:100, y = rnorm(length(x)), ...)\r\nplot_simulated_data(data.sim, ylims = c(0, 30), ...)\r\nwrite_results_into_table(data.sim, outFile=\"output.xlsx\")\r\n\r\n\r\n\r\nIn the refactorized version each sub-block was put into a separate function. In contrast to the single-block version, each of these functions can be re-used, tested and have their own documentation. Since each of such functions encapsulate their own environment, the refactorized design is also less vulnerable to side-effects between blocks. A second person can now read and understand function by function without having to worry about the rest of the block.\r\nLast but not least, the block comments in the single-block versions could be transformed into function names so that the documentation is now part of the code and as such no longer can get out of sync with it.\r\nPackages and namespaces\r\nWhenever the :: operator is used, the namespace of the corresponding package is loaded but not attached to the search path.\r\n\r\n\r\ntools::file_ext(\"test.txt\")     # loads the namespace of the 'tools' package,\r\n\r\n\r\n[1] \"txt\"\r\n\r\nsearch()                        # but does not attach it to the search path\r\n\r\n\r\n[1] \".GlobalEnv\"        \"package:stats\"     \"package:graphics\" \r\n[4] \"package:grDevices\" \"package:utils\"     \"package:datasets\" \r\n[7] \"package:methods\"   \"Autoloads\"         \"package:base\"     \r\n\r\nfile_ext(\"test.txt\")  # and thus gives error if called without namespace prefix\r\n\r\n\r\nError in file_ext(\"test.txt\"): could not find function \"file_ext\"\r\n\r\n# base::mean and stats::rnorm work, because base and stats namespaces are\r\n# loaded and attached by default:\r\nmean(rnorm(10))\r\n\r\n\r\n[1] 0.05354217\r\n\r\nIn contrast, the library and require commands both load the package’s namespace but also attach its namespace to the search path, which allows to refer to functions of the package without using the :: operator.\r\n\r\n\r\nlibrary(tools)                # loads namespace and attaches it to search path\r\nsearch()\r\n\r\n\r\n [1] \".GlobalEnv\"        \"package:tools\"     \"package:stats\"    \r\n [4] \"package:graphics\"  \"package:grDevices\" \"package:utils\"    \r\n [7] \"package:datasets\"  \"package:methods\"   \"Autoloads\"        \r\n[10] \"package:base\"     \r\n\r\nfile_ext(\"test.txt\")          # now works\r\n\r\n\r\n[1] \"txt\"\r\n\r\nSince a call to a function shall not alter the search path, library or require statements are not allowed in functions used in R packages. In contrast, library statements are suitable for local (data analysis) R scripts especially if a specific function is used frequently. An alternative is to locally re-map a frequently used function:\r\n\r\n\r\nget_file_extension <- tools::file_ext\r\nget_file_extension(\"test.txt\")\r\n\r\n\r\n[1] \"txt\"\r\n\r\nget_file_extension(\"test.docx\")\r\n\r\n\r\n[1] \"docx\"\r\n\r\nget_file_extension(\"test.xlsx\")\r\n\r\n\r\n[1] \"xlsx\"\r\n\r\nCode documentation\r\nFunction headers\r\nA function header is placed above any function, unless it is defined inside another function.\r\nIt is recommended to use the roxygen format, because it\r\npromotes a standardized documentation\r\nallows for automatic creation of a user-documentation from the header\r\nallows for automatic creation of all namespace definitions of an R-package\r\nA function header at least contains the following elements (the corresponding roxygen keyword is listed at the start):\r\n@title: short sentence of what the function does\r\n@description: extended description of the function (optionally the @details keyword can be used to describe further details)\r\n@param (or @field with RefClasses): For each input parameter, a summary of the type of the parameter (e.g., string, numeric vector) and, if not obvious from the name, what the parameter does.\r\n@return: describes the output from the function, if it returns something.\r\n@examples: if applicable, examples of function calls are provided. Providing executable R code, which shows how to use the function in practice, is a very important part of the documentation, because people usually look at the examples first. While generally example code should work without errors, for the purpose of illustration, it is often useful to also include code that causes an error. If done, the corresponding place in the code should be marked accordingly (use  with roxygen).\r\nExample of a roxygen-header:\r\n\r\n\r\n#' @title String suffix matching\r\n#'\r\n#' @description\r\n#' Determines whether \\code{end} is a suffix of string \\code{s} (borrowed from\r\n#' Python, where it would read \\code{s.endswith(end)})\r\n#'\r\n#' @param s (character) the input character string\r\n#' @param end (character) string to be checked whether it is a suffix of the\r\n#' input string \\code{s}.\r\n#' @return \\code{TRUE} if \\code{end} is a suffix of \\code{s} else \\code{FALSE}\r\n#'\r\n#' @examples\r\n#' string_ends_with(\"Hello World!\", \"World!\")   # TRUE\r\n#' string_ends_with(\" Hello World!\", \"world!\")  # FALSE (case sensitive)\r\nstring_ends_with <- function(s, end)\r\n{\r\n    # Implementation ...\r\n}\r\n\r\n\r\n\r\nInline code comments\r\nInline comments should explain the programmer’s intent at a higher level of abstraction than the code, that is, they should provide additional information, which are not obvious from reading the code alone. As such, good comments don’t repeat, summarize or explain the code, unless the code is so complicated that it warrants an explanation, in which case, however, it is often worth to revise the code to make it more readable instead.\r\nExamples of suitable, informative comments:\r\n\r\n\r\n# Compare strings pairwise and determine first position of differing characters\r\nsplitted_s <- strsplit(s, split = \"\")[[1]]\r\nsplitted_url <- strsplit(url, split = \"\")[[1]][1:nchar(s)]\r\ndifferent <- splitted_s != splitted_url\r\nfirst_different_position <- which(different)[1]\r\n\r\n# Provide index via names as we need them later\r\nnames(v) <- seq_along(v)\r\n\r\n\r\n\r\nBad redundant comments:\r\n\r\n\r\nv <- 1:10  # initialize vector\r\n\r\n# Loop through all numbers in the vector and increment by one\r\nfor (i in 1:length(v)) {\r\n    v[i] <- v[i] + 1  # increment number\r\n}\r\n\r\n\r\n\r\nThat’s it - happy coding!\r\n\r\n\r\n\r\n",
    "preview": "posts/r-style-guide/distill-preview.png",
    "last_modified": "2022-06-17T15:30:58+02:00",
    "input_file": {},
    "preview_width": 258,
    "preview_height": 191
  },
  {
    "path": "posts/readable-code-part1/",
    "title": "Readable code with base R (part 1)",
    "description": "Producing readable R code is of great importance, especially if there is a \nchance that you will share your code with people other than your future self.\nIn this series of blog posts, I will present some (often underused) base R \nfunctions for this purpose.",
    "author": [],
    "date": "2018-07-30",
    "categories": [
      "base R",
      "coding style"
    ],
    "contents": "\r\n\r\n\r\n\r\nIn this post, we cover startsWith, endsWith, and Filter.\r\nstartsWith and endsWith for string-matching\r\nThere are special base functions for pre- or postfix matching.\r\n\r\n\r\n# Basic usage:\r\nw <- \"Hello World!\"\r\nstartsWith(w, \"Hell\")\r\n\r\n\r\n[1] TRUE\r\n\r\nstartsWith(w, \"Helo\")\r\n\r\n\r\n[1] FALSE\r\n\r\nendsWith(w, \"!\")\r\n\r\n\r\n[1] TRUE\r\n\r\nOf course, it also works with vectors. Can’t remember the exact name of a base function? Try this… ;)\r\n\r\n\r\nbase_funcs <- ls(\"package:base\")\r\n\r\nbase_funcs[startsWith(base_funcs, \"row\")]\r\n\r\n\r\n [1] \"row\"                    \"row.names\"             \r\n [3] \"row.names.data.frame\"   \"row.names.default\"     \r\n [5] \"row.names<-\"            \"row.names<-.data.frame\"\r\n [7] \"row.names<-.default\"    \"rowMeans\"              \r\n [9] \"rownames\"               \"rownames<-\"            \r\n[11] \"rowsum\"                 \"rowsum.data.frame\"     \r\n[13] \"rowsum.default\"         \"rowSums\"               \r\n\r\nThe ‘readable’ property really shines when combined with control-flow.\r\n\r\n\r\ntell_file_type <- function(fn) {\r\n    # Check different file endings\r\n    if (endsWith(fn, \"txt\")) {\r\n        print(\"A text file.\")\r\n    }\r\n    if (any(endsWith(fn, c(\"xlsx\", \"xls\")))) {\r\n        print(\"An Excel file.\")\r\n    }\r\n}\r\ntell_file_type(\"A.txt\")\r\n\r\n\r\n[1] \"A text file.\"\r\n\r\ntell_file_type(\"B.xls\")\r\n\r\n\r\n[1] \"An Excel file.\"\r\n\r\nThe resulting code reads very well.\r\nFilter\r\nUsing another nice base function, Filter, the above code can be further improved.\r\n\r\n\r\nget_file_type <- function(fn) {\r\n  file_endings <- c(text=\"txt\", Excel=\"xls\", Excel=\"xlsx\")  \r\n  Filter(file_endings, f = function(x) endsWith(fn, x))\r\n}\r\n\r\nget_file_type(\"C.xlsx\")\r\n\r\n\r\n Excel \r\n\"xlsx\" \r\n\r\nAgain, very readable to my eyes. It should be noted that for this particular problem using tools::file_ext is even more appropriate, but I think the point has been made.\r\nLast but not least, since Filter works on lists, you can use it on a data.frame as well.\r\n\r\n\r\ndat <- data.frame(A=1:3, B=5:3, L=letters[1:3])\r\ndat\r\n\r\n\r\n  A B L\r\n1 1 5 a\r\n2 2 4 b\r\n3 3 3 c\r\n\r\nFilter(dat, f = is.numeric)\r\n\r\n\r\n  A B\r\n1 1 5\r\n2 2 4\r\n3 3 3\r\n\r\nFilter(dat, f = Negate(is.numeric))  # or Filter(dat, f = function(x) !is.numeric(x))\r\n\r\n\r\n  L\r\n1 a\r\n2 b\r\n3 c\r\n\r\nThat’s it for now - see you in part 2.\r\n\r\n\r\n\r\n",
    "preview": "posts/readable-code-part1/distill-preview.png",
    "last_modified": "2022-06-17T15:30:58+02:00",
    "input_file": {},
    "preview_width": 569,
    "preview_height": 322
  }
]
